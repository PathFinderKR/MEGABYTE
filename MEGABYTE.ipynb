{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "52896242a346d463"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.188951Z",
     "start_time": "2024-10-11T06:53:42.344565Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Huggingface\n",
    "import huggingface_hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Weights & Biases\n",
    "import wandb"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "id": "d389d6b8bf136b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.192055Z",
     "start_time": "2024-10-11T06:53:45.190161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch size = P\n",
    "# context size = T\n",
    "# Number of patches = K = T/P\n",
    "# global embedding dimension = d_G\n",
    "# local embedding dimension = d_L"
   ],
   "id": "c4ef3d46e76a5317",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.197890Z",
     "start_time": "2024-10-11T06:53:45.192803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = True\n",
    "    \n",
    "    # Model\n",
    "    V: int = 512  # 258 utf-8 characters + 2 special tokens\n",
    "    P: int = 4\n",
    "    T: int = 1024\n",
    "    K: int = T // P  # Number of patches\n",
    "    \n",
    "    model_name: str = 'MEGABYTE'\n",
    "    model_size: str = 'small'  # 'small' or 'large'\n",
    "    \n",
    "    ## Small\n",
    "    if model_size == 'small':\n",
    "        ### Global model\n",
    "        n_layers_G: int = 6\n",
    "        n_heads_G: int = 4\n",
    "        d_G: int = 128\n",
    "        d_head_G: int = d_G // n_heads_G\n",
    "        d_ff_G: int = d_G * 4\n",
    "        dropout_G: float = 0.1\n",
    "        ### Local model\n",
    "        n_layers_L: int = 4\n",
    "        n_heads_L: int = 4\n",
    "        d_L: int = 64\n",
    "        d_head_L: int = d_L // n_heads_L\n",
    "        d_ff_L: int = d_L * 4\n",
    "        dropout_L: float = 0.1\n",
    "    ### Large\n",
    "    elif model_size == 'large':\n",
    "        ### Global model\n",
    "        n_layers_G: int = 12\n",
    "        n_heads_G: int = 8\n",
    "        d_G: int = 256\n",
    "        d_head_G: int = d_G // n_heads_G\n",
    "        d_ff_G: int = d_G * 4\n",
    "        dropout_G: float = 0.1\n",
    "        ### Local model\n",
    "        n_layers_L: int = 8\n",
    "        n_heads_L: int = 8\n",
    "        d_L: int = 128\n",
    "        d_head_L: int = d_L // n_heads_L\n",
    "        d_ff_L: int = d_L * 4\n",
    "        dropout_L: float = 0.2\n",
    "    \n",
    "    flash_attention: bool = False\n",
    "    \n",
    "    # Vocabulary\n",
    "    PAD_ID: int = 256\n",
    "    EOS_ID: int = 257\n",
    "    \n",
    "    # data\n",
    "    validation_size: float = 0.2\n",
    "    shakespeare_id = \"data/shakespeare.txt\"\n",
    "    wiki_id = \"wikimedia/wikipedia\"\n",
    "    dataset_id = wiki_id\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 2e-5 # 5e-4 ~ 1e-6\n",
    "    \n",
    "    # Generation\n",
    "    max_len: int = 8192\n",
    "    temperature: float = 1.0\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 101"
   ],
   "id": "60e93c2e04de8080",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "13c03a9d7d545aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.204217Z",
     "start_time": "2024-10-11T06:53:45.199261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "fce8043802833d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 101\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "aef10a529a5447d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.220449Z",
     "start_time": "2024-10-11T06:53:45.205153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "39f348183c5da647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on MPS\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug",
   "id": "a405beab399da1ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.223217Z",
     "start_time": "2024-10-11T06:53:45.221253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.model_size = 'small'\n",
    "    CONFIG.dataset_id = \"data/shakespeare.txt\"\n",
    "    CONFIG.epochs = 1\n",
    "    CONFIG.max_len = 2000"
   ],
   "id": "c18def8c7828e0ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HuggingFace",
   "id": "df7bcab560d43bd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.225914Z",
     "start_time": "2024-10-11T06:53:45.223924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    load_dotenv()\n",
    "    token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "    huggingface_hub.login(token=token, add_to_git_credential=True)"
   ],
   "id": "268ac7fe97641cca",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Weights & Biases",
   "id": "9b1b365edf72fb83"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.228596Z",
     "start_time": "2024-10-11T06:53:45.226502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not CONFIG.debug:\n",
    "    api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "    wandb.login(key=api_key)\n",
    "    wandb.init(project=CONFIG.model_name)"
   ],
   "id": "dbc4edc055dd6044",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "fe4f42ed478346e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#asdf",
   "id": "2ebf95da965b1a53"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Shakespeare",
   "id": "46bc03f62c2f8dd5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.232495Z",
     "start_time": "2024-10-11T06:53:45.229328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_shakespeare():\n",
    "    with open(CONFIG.shakespeare_id, 'r') as f:\n",
    "        shakespeare_text = f.read()\n",
    "    return shakespeare_text\n",
    "\n",
    "shakespeare_text = load_shakespeare()"
   ],
   "id": "233545ec5cf61b5e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.237259Z",
     "start_time": "2024-10-11T06:53:45.235027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(shakespeare_text[:1000])\n",
    "print(f'Total number of characters in the text: {len(shakespeare_text)}')"
   ],
   "id": "4e9a6798598cc0d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n",
      "Total number of characters in the text: 1115394\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Wikipedia",
   "id": "969f0de13b5fcfa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.239502Z",
     "start_time": "2024-10-11T06:53:45.237958Z"
    }
   },
   "cell_type": "code",
   "source": "#wiki_dataset = load_dataset(CONFIG.wiki_id, \"20231101.en\")",
   "id": "194b62588de0b623",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenization",
   "id": "505c578a5ddc11b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.242685Z",
     "start_time": "2024-10-11T06:53:45.240061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char2int = {chr(i): i for i in range(CONFIG.V)}\n",
    "int2char = {i: chr(i) for i in range(CONFIG.V)}\n",
    "\n",
    "# Add special tokens\n",
    "char2int['<PAD>'] = CONFIG.PAD_ID\n",
    "int2char[CONFIG.PAD_ID] = '<PAD>'\n",
    "char2int['<EOS>'] = CONFIG.EOS_ID\n",
    "int2char[CONFIG.EOS_ID] = '<EOS>'\n",
    "\n",
    "# Encoding and Decoding\n",
    "encode = lambda text: [char2int[c] for c in text]\n",
    "decode = lambda tokens: ''.join([int2char[t] for t in tokens])"
   ],
   "id": "7bf3e3f8386331c9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.245175Z",
     "start_time": "2024-10-11T06:53:45.243412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = \"Hello, World!\" + \"<EOS>\"\n",
    "sample_tokens = encode(sample_text)\n",
    "print(sample_tokens)\n",
    "print(decode(sample_tokens))"
   ],
   "id": "fb83b51bf61a3dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33, 60, 69, 79, 83, 62]\n",
      "Hello, World!<EOS>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "a16efac757bb6828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.327633Z",
     "start_time": "2024-10-11T06:53:45.245892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(dataset_id):\n",
    "    if dataset_id == CONFIG.shakespeare_id:\n",
    "        text = load_shakespeare()\n",
    "    elif dataset_id == CONFIG.wiki_id:\n",
    "        text = wiki_dataset['train']['text']\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset id\")\n",
    "    tokens = torch.tensor(encode(text), dtype=torch.long)\n",
    "    return tokens\n",
    "\n",
    "dataset_tokens = preprocess(CONFIG.dataset_id)"
   ],
   "id": "9d8a3ca8cb3e0023",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.331273Z",
     "start_time": "2024-10-11T06:53:45.328743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_validation_split(tokens, validation_size):\n",
    "    train_size = int(len(tokens) * (1 - validation_size))\n",
    "    return tokens[:train_size], tokens[train_size:]\n",
    "\n",
    "train_tokens, validation_tokens = train_validation_split(dataset_tokens, CONFIG.validation_size)"
   ],
   "id": "846af67d59750175",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.333877Z",
     "start_time": "2024-10-11T06:53:45.331965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Train size: {len(train_tokens)}')\n",
    "print(f'Validation size: {len(validation_tokens)}')"
   ],
   "id": "14ac4046d6eb5ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 892315\n",
      "Validation size: 223079\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.337334Z",
     "start_time": "2024-10-11T06:53:45.335185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = tokens\n",
    "        self.context_length = context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.context_length], self.tokens[idx+1:idx+self.context_length+1]"
   ],
   "id": "f8d54da75993d1e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.340343Z",
     "start_time": "2024-10-11T06:53:45.338037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataloader(train_tokens, validation_tokens, context_length, batch_size):\n",
    "    train_dataset = TextDataset(train_tokens, context_length)\n",
    "    validation_dataset = TextDataset(validation_tokens, context_length)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, validation_loader\n",
    "\n",
    "train_loader, validation_loader = create_dataloader(train_tokens, validation_tokens, CONFIG.T, CONFIG.batch_size)"
   ],
   "id": "bf526d152283658f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.706436Z",
     "start_time": "2024-10-11T06:53:45.341009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x, sample_y = sample_x.to(CONFIG.device), sample_y.to(CONFIG.device)\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(sample_x[0])\n",
    "print(sample_y[0])"
   ],
   "id": "aa19cb9b5d0353c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1024]) torch.Size([128, 1024])\n",
      "tensor([101, 110, 115,  ...,  97, 110,  32], device='mps:0')\n",
      "tensor([110, 115, 107,  ..., 110,  32, 119], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c45df9ae58cabd96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "36e603385ec68491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.711863Z",
     "start_time": "2024-10-11T06:53:45.707348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_heads: int, d_head: int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.query = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.key = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.value = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.out = nn.Linear(self.n_heads * self.d_head, self.d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        batch_size, context_size, _ = x.size()\n",
    "        q = self.query(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        k = self.key(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        v = self.value(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        \n",
    "        q = q.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        k = k.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        v = v.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        \n",
    "        # Masked Self Attention\n",
    "        mask = torch.triu(torch.ones(context_size, context_size, device=x.device), diagonal=1).bool()  # [context_size, context_size]\n",
    "        mask = mask.view(1, 1, context_size, context_size)  # [1, 1, context_size, context_size]\n",
    "        mask = mask.repeat(batch_size, self.n_heads, 1, 1)  # [batch_size, n_heads, context_size, context_size]\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)  # [batch_size, n_heads, context_size, context_size]\n",
    "        scores = scores.masked_fill(mask, float('-inf'))  # [batch_size, n_heads, context_size, context_size]\n",
    "        scores = F.softmax(scores, dim=-1)  # [batch_size, n_heads, context_size, context_size]\n",
    "        \n",
    "        x = torch.matmul(scores, v)  # [batch_size, n_heads, context_size, d_head]\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, context_size, self.n_heads * self.d_head)  # [batch_size, context_size, n_heads * d_head]\n",
    "        x = self.out(x)  # [batch_size, context_size, d_embed]\n",
    "        return x"
   ],
   "id": "2c4f95af6a999a2a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.716919Z",
     "start_time": "2024-10-11T06:53:45.713570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_embed, d_ff, bias=False)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        x = F.gelu(self.fc1(x))  # [batch_size, context_size, d_ff]\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)  # [batch_size, context_size, d_embed]\n",
    "        return x"
   ],
   "id": "623ef725d6ec7186",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.721266Z",
     "start_time": "2024-10-11T06:53:45.717983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_head: int, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = CausalSelfAttention(d_embed=d_embed, n_heads=n_heads, d_head=d_head)\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "        self.mlp = MLP(d_embed=d_embed, d_ff=d_ff, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.attention(self.norm1(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.mlp(self.norm2(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "2ca7331d305226e7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MEGABYTE",
   "id": "ca7d1e714568e5cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.724567Z",
     "start_time": "2024-10-11T06:53:45.721997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_G)\n",
    "        self.positional_embedding = nn.Embedding(config.T, config.d_G)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        assert self.config.T % self.config.P == 0, \"context size must be divisible by patch size\"\n",
    "        \n",
    "        bytes = self.embedding(bytes) + self.positional_embedding(torch.arange(self.config.T, device=bytes.device))  # [batch_size, context_size, d_embed]\n",
    "        bytes = rearrange(bytes, \"b (k p) d -> b k (p d)\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        return bytes"
   ],
   "id": "aca577b5d80002f1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.727960Z",
     "start_time": "2024-10-11T06:53:45.725259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedder = PatchEmbedder(config)\n",
    "        self.decoder = Decoder(n_heads=config.n_heads_G, d_head=config.d_head_G, d_embed=config.P*config.d_G, d_ff=config.d_ff_G, dropout=config.dropout_G)\n",
    "        self.linear = nn.Linear(config.d_G, config.d_L, bias=False)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        x = self.patch_embedder(bytes)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        for _ in range(self.config.n_layers_G):\n",
    "            x = self.decoder(x)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        x = rearrange(x, \"b k (p d) -> (b k) p d\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size * num_patches, patch_size, d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "4962881ec29397b7",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.731602Z",
     "start_time": "2024-10-11T06:53:45.728687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_L)\n",
    "        self.local_transformer = Decoder(n_heads=config.n_heads_L, d_head=config.d_head_L, d_embed=config.d_L, d_ff=config.d_ff_L, dropout=config.dropout_L)\n",
    "        self.linear = nn.Linear(config.d_L, config.V, bias=False)\n",
    "        \n",
    "    def forward(self, local_input, global_output):  # [batch_size * num_patches, patch_size], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.embedding(local_input) + global_output  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        for _ in range(self.config.n_layers_L):\n",
    "            x = self.local_transformer(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, vocab_size]\n",
    "        x = rearrange(x, \"(b k) p v -> b (k p) v\", k=self.config.K, p=self.config.P, v=self.config.V)  # [batch_size, context_size, vocab_size]\n",
    "        return x"
   ],
   "id": "98d05f3337de7d5b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.737923Z",
     "start_time": "2024-10-11T06:53:45.732371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MEGABYTE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.global_model = GlobalModel(config)\n",
    "        self.local_model = LocalModel(config)\n",
    "        self.max_len = config.max_len\n",
    "        self.context_size = config.T\n",
    "        self.temperature = config.temperature\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        global_input, local_input = self.prepare_input(bytes)  # [batch_size, context_size], [batch_size * num_patches, patch_size]\n",
    "        global_output = self.global_model(global_input)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        local_output = self.local_model(local_input, global_output)  # [batch_size, context_size, vocab_size]\n",
    "        return local_output\n",
    "        \n",
    "    def prepare_input(self, bytes):  # [batch_size, context_size]\n",
    "        global_padding = bytes.new(bytes.shape[0], self.config.P).fill_(self.config.PAD_ID)  # [batch_size, patch_size]\n",
    "        global_input = torch.cat((global_padding, bytes[:, :-self.config.P]), dim=-1)  # [batch_size, context_size]\n",
    "        \n",
    "        bytes_input = rearrange(bytes, \"b (k p) -> (b k) p\", p=self.config.P)  # [batch_size * num_patches, patch_size]\n",
    "        local_padding = bytes_input.new(bytes_input.shape[0], 1).fill_(self.config.PAD_ID)  # [patch_size]\n",
    "        local_input = torch.cat((local_padding, bytes_input[:, :-1]), dim=-1)  # [batch_size * num_patches, patch_size]\n",
    "        return global_input, local_input\n",
    "    \n",
    "    def loss(self, bytes, y):  # y: [batch_size, context_size]\n",
    "        y = rearrange(y, \"b t -> (b t)\")  # [batch_size * context_size]\n",
    "        logits = self.forward(bytes)  # [batch_size, context_size, vocab_size]\n",
    "        logits = rearrange(logits, \"b t v -> (b t) v\", v=self.config.V)  # [batch_size * context_size, vocab_size]\n",
    "        return F.cross_entropy(logits, y, ignore_index=self.config.PAD_ID)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, bytes, max_len=None, decode_fn=None):\n",
    "        self.eval()\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "    \n",
    "        for _ in range(max_len - bytes.size(1)):  # x: [batch_size, context]\n",
    "            context = bytes[:, -self.context_size:]  # [batch_size, context_size]\n",
    "            output = self.forward(context)  # [batch_size, context_size, vocab_size]\n",
    "            logits = output[:, -1, :] / self.temperature\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(-1)  # [batch_size]\n",
    "            bytes = torch.cat((bytes, next_token.unsqueeze(-1)), dim=-1)  # [batch_size, context]\n",
    "    \n",
    "            # Decode token\n",
    "            if decode_fn is not None:\n",
    "                decoded_token = decode_fn([next_token[0].item()])\n",
    "                print(decoded_token, end='', flush=True)\n",
    "                \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ],
   "id": "625b775dda52818a",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:53:45.756489Z",
     "start_time": "2024-10-11T06:53:45.738759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "megabyte = MEGABYTE(CONFIG).to(CONFIG.device)\n",
    "print(megabyte)"
   ],
   "id": "4b7a8d3a9155e231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGABYTE(\n",
      "  (global_model): GlobalModel(\n",
      "    (patch_embedder): PatchEmbedder(\n",
      "      (embedding): Embedding(512, 128)\n",
      "      (positional_embedding): Embedding(1024, 128)\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (key): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (value): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (out): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=False)\n",
      "  )\n",
      "  (local_model): LocalModel(\n",
      "    (embedding): Embedding(512, 64)\n",
      "    (local_transformer): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out): Linear(in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:54:41.032298Z",
     "start_time": "2024-10-11T06:53:45.760740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = megabyte.loss(sample_x, sample_y)\n",
    "print(loss)\n",
    "\n",
    "megabyte.generate(sample_x, max_len=2048, decode_fn=decode)"
   ],
   "id": "a2e9cd2168cc8fa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9199, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "Ş>ƣå(İČXŻÈƎŁ©śƺƿâÝÏôVǺQ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "7edb7b78f1714e9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:54:41.047534Z",
     "start_time": "2024-10-11T06:54:41.046201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model):\n",
    "    model = model.to(CONFIG.device)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    for epoch in range(CONFIG.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CONFIG.epochs}'):\n",
    "            x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss.append(running_loss / len(train_loader))\n",
    "        print(f'Training Loss: {running_loss / len(train_loader)}')\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "                loss = model.loss(x, y)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "        validation_loss.append(running_loss / len(validation_loader))\n",
    "        print(f'Validation Loss: {running_loss / len(validation_loader)}')\n",
    "        \n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "1b495cc61a877c49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:54:41.060165Z",
     "start_time": "2024-10-11T06:54:41.054187Z"
    }
   },
   "cell_type": "code",
   "source": "train(model=megabyte)",
   "id": "3d97d7626c626591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e70edc36e643819e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:54:41.080674Z",
     "start_time": "2024-10-11T06:54:41.068049Z"
    }
   },
   "cell_type": "code",
   "source": "megabyte.generate(sample_x, decode_fn=decode)",
   "id": "c5067e98aebbc674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "495822436e557079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-11T06:54:41.117659Z",
     "start_time": "2024-10-11T06:54:41.110776Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "acd82062c9902c0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

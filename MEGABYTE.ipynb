{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "52896242a346d463"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.116553Z",
     "start_time": "2024-10-08T16:45:45.186027Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "id": "d389d6b8bf136b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.120183Z",
     "start_time": "2024-10-08T16:45:46.117929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch size = P\n",
    "# Sequence length = T\n",
    "# Number of patches = K = T/P\n",
    "# global embedding dimension = Dg\n",
    "# local embedding dimension = Dl"
   ],
   "id": "c4ef3d46e76a5317",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.129742Z",
     "start_time": "2024-10-08T16:45:46.121118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    V: int = 512  # 256 ASCII characters + 2 special tokens\n",
    "    P: int = 4\n",
    "    T: int = 1024\n",
    "    K: int = T // P  # Number of patches\n",
    "    \n",
    "    ## Global model\n",
    "    D_G: int = 256\n",
    "    n_layers_G: int = 8\n",
    "    n_heads_G: int = 16\n",
    "    d_head_G: int = D_G // n_heads_G\n",
    "    d_ff_G: int = D_G * 4\n",
    "    dropout_G: float = 0.2\n",
    "    \n",
    "    ## Local model\n",
    "    D_L: int = 128\n",
    "    n_layers_L: int = 4\n",
    "    n_heads_L: int = 8\n",
    "    d_head_L: int = D_L // n_heads_L\n",
    "    d_ff_L: int = D_L * 4\n",
    "    dropout_L: float = 0.2\n",
    "    \n",
    "    flash_attention: bool = False\n",
    "    \n",
    "    # Vocabulary\n",
    "    PAD_ID: int = 256\n",
    "    EOS_ID: int = 257\n",
    "    \n",
    "    # data\n",
    "    validation_size: float = 0.2\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    \n",
    "    # Generation\n",
    "    max_len: int = 1024\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 42"
   ],
   "id": "60e93c2e04de8080",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "13c03a9d7d545aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.140921Z",
     "start_time": "2024-10-08T16:45:46.131003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "fce8043802833d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "aef10a529a5447d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.417200Z",
     "start_time": "2024-10-08T16:45:46.141748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "39f348183c5da647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug",
   "id": "a405beab399da1ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.420501Z",
     "start_time": "2024-10-08T16:45:46.418123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.epochs = 1"
   ],
   "id": "c18def8c7828e0ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "fe4f42ed478346e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.427752Z",
     "start_time": "2024-10-08T16:45:46.421393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data path\n",
    "dataset_path = 'data/'\n",
    "# shakespeare data\n",
    "shakespeare_dataset = dataset_path + 'shakespeare.txt'"
   ],
   "id": "74c5f6ded01e7533",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.440210Z",
     "start_time": "2024-10-08T16:45:46.428718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the dataset\n",
    "with open(shakespeare_dataset, 'r', encoding='utf-8') as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "id": "31b5b3b8de611ece",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.443872Z",
     "start_time": "2024-10-08T16:45:46.441388Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:1000])",
   "id": "4e9a6798598cc0d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.450116Z",
     "start_time": "2024-10-08T16:45:46.445464Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Total number of characters in the text: {len(shakespeare_text)}')",
   "id": "e851ba99cbf035c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 1115394\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.456569Z",
     "start_time": "2024-10-08T16:45:46.450986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(text):\n",
    "    return [char for char in text.encode('utf-8')]\n",
    "\n",
    "def decode(encoded_text):\n",
    "    return bytes(encoded_text).decode('utf-8')"
   ],
   "id": "611476b40babf189",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.463654Z",
     "start_time": "2024-10-08T16:45:46.457434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(encode('Hello, World!'))\n",
    "print(decode(encode('Hello, World!')))"
   ],
   "id": "fb83b51bf61a3dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.546894Z",
     "start_time": "2024-10-08T16:45:46.464547Z"
    }
   },
   "cell_type": "code",
   "source": "shakespeare_tokens = torch.tensor(encode(shakespeare_text))",
   "id": "c2f9b47e50c0575b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "a16efac757bb6828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.554278Z",
     "start_time": "2024-10-08T16:45:46.547854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Validation Split\n",
    "train_size = int(len(shakespeare_tokens) * (1 - CONFIG.validation_size))\n",
    "train_tokens = shakespeare_tokens[:train_size]\n",
    "validation_tokens = shakespeare_tokens[train_size:]\n",
    "print(f'Total number of tokens in the training set: {len(train_tokens)}')\n",
    "print(f'Total number of tokens in the validation set: {len(validation_tokens)}')"
   ],
   "id": "2d8b1d2c86b75d06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the training set: 892315\n",
      "Total number of tokens in the validation set: 223079\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.560887Z",
     "start_time": "2024-10-08T16:45:46.555313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = tokens\n",
    "        self.context_length= context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.context_length], self.tokens[idx+1:idx+self.context_length+1]\n",
    "\n",
    "train_dataset = ShakespeareDataset(train_tokens, CONFIG.T)\n",
    "validation_dataset = ShakespeareDataset(validation_tokens, CONFIG.T)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.batch_size, shuffle=False)"
   ],
   "id": "10bccc9360cffba3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.739241Z",
     "start_time": "2024-10-08T16:45:46.561686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x, sample_y = sample_x.to(CONFIG.device), sample_y.to(CONFIG.device)\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(sample_x[0])\n",
    "print(sample_y[0])"
   ],
   "id": "36aefea017f1c78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024]) torch.Size([16, 1024])\n",
      "tensor([101, 101, 100,  ...,  10,  10,  66], device='cuda:0')\n",
      "tensor([101, 100,  32,  ...,  10,  66,  69], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c45df9ae58cabd96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.743472Z",
     "start_time": "2024-10-08T16:45:46.740222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_head: int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.query = nn.Linear(self.d_embed, self.d_head)\n",
    "        self.key = nn.Linear(self.d_embed, self.d_head)\n",
    "        self.value = nn.Linear(self.d_embed, self.d_head)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, block_size, d_embed]\n",
    "        query = self.query(x)  # [batch_size, block_size, d_head]\n",
    "        key = self.key(x)  # [batch_size, block_size, d_head]\n",
    "        value = self.value(x)  # [batch_size, block_size, d_head]\n",
    "        \n",
    "        attention = torch.matmul(query, key.transpose(-2, -1)) / np.sqrt(self.d_head)  # [batch_size, block_size, block_size]\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        x = torch.matmul(attention, value)  # [batch_size, block_size, d_head]\n",
    "        return x"
   ],
   "id": "4dd49369035d6ac8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.754318Z",
     "start_time": "2024-10-08T16:45:46.744550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_head: int, d_embed: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(d_embed=d_embed, d_head=d_head) for _ in range(n_heads)])\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, block_size, d_embed]\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)  # [batch_size, block_size, d_embed]"
   ],
   "id": "119338731441e157",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.761060Z",
     "start_time": "2024-10-08T16:45:46.755247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_embed, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))  # [batch_size, block_size, d_ff]\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)  # [batch_size, block_size, d_embed]\n",
    "        return x"
   ],
   "id": "623ef725d6ec7186",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.767994Z",
     "start_time": "2024-10-08T16:45:46.762016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_head: int, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads=n_heads, d_head=d_head, d_embed=d_embed)\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "        self.mlp = MLP(d_embed=d_embed, d_ff=d_ff, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, block_size, d_embed]\n",
    "        x = x + self.attention(self.norm1(x))  # [batch_size, block_size, d_embed]\n",
    "        x = x + self.mlp(self.norm2(x))  # [batch_size, block_size, vocab_size]\n",
    "        return x"
   ],
   "id": "2ca7331d305226e7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.776345Z",
     "start_time": "2024-10-08T16:45:46.768876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(CONFIG.V, CONFIG.D_G)\n",
    "        self.positional_embedding = nn.Embedding(CONFIG.T, CONFIG.D_G)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, sequence_length]\n",
    "        assert CONFIG.T % CONFIG.P == 0, \"Sequence length must be divisible by patch size\"\n",
    "        \n",
    "        bytes = self.embedding(bytes) + self.positional_embedding(torch.arange(CONFIG.T, device=bytes.device))  # [batch_size, sequence_len, d_embed]\n",
    "        bytes = rearrange(bytes, \"b (k p) d -> b k (p d)\", b=bytes.shape[0], k=CONFIG.K, p=CONFIG.P, d=CONFIG.D_G)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        return bytes"
   ],
   "id": "aca577b5d80002f1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.783546Z",
     "start_time": "2024-10-08T16:45:46.777305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedder = PatchEmbedder()\n",
    "        self.decoder = Decoder(n_heads=CONFIG.n_heads_G, d_head=CONFIG.d_head_G, d_embed=CONFIG.P*CONFIG.D_G, d_ff=CONFIG.d_ff_G, dropout=CONFIG.dropout_G)\n",
    "        self.linear = nn.Linear(CONFIG.D_G, CONFIG.D_L)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, sequence_length]\n",
    "        x = self.patch_embedder(bytes)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        for _ in range(CONFIG.n_layers_G):\n",
    "            x = self.decoder(x)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        x = rearrange(x, \"b k (p d) -> (b k) p d\", b=bytes.shape[0], k=CONFIG.K, p=CONFIG.P, d=CONFIG.D_G)  # [batch_size * num_patches, patch_size, d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "4962881ec29397b7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.792866Z",
     "start_time": "2024-10-08T16:45:46.784563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(CONFIG.V, CONFIG.D_L)\n",
    "        self.local_transformer = Decoder(n_heads=CONFIG.n_heads_L, d_head=CONFIG.d_head_L, d_embed=CONFIG.D_L, d_ff=CONFIG.d_ff_L, dropout=CONFIG.dropout_L)\n",
    "        self.linear = nn.Linear(CONFIG.D_L, CONFIG.V)\n",
    "        \n",
    "    def forward(self, local_input, global_output):  # [batch_size * num_patches, patch_size], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.embedding(local_input) + global_output  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        for _ in range(CONFIG.n_layers_L):\n",
    "            x = self.local_transformer(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, vocab_size]\n",
    "        x = rearrange(x, \"(b k) p v -> b (k p) v\", k=CONFIG.K, p=CONFIG.P, v=CONFIG.V)  # [batch_size, sequence_length, vocab_size]\n",
    "        return x"
   ],
   "id": "98d05f3337de7d5b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.800300Z",
     "start_time": "2024-10-08T16:45:46.793727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MEGABYTE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.global_model = GlobalModel()\n",
    "        self.local_model = LocalModel()\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, sequence_length]\n",
    "        global_input, local_input = self.prepare_input(bytes)  # [batch_size, sequence_length], [batch_size * num_patches, patch_size]\n",
    "        global_output = self.global_model(global_input)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        local_output = self.local_model(local_input, global_output)  # [batch_size, sequence_length, vocab_size]\n",
    "        return local_output\n",
    "        \n",
    "    def prepare_input(self, bytes):  # [batch_size, sequence_length]\n",
    "        global_padding = bytes.new(bytes.shape[0], CONFIG.P).fill_(CONFIG.PAD_ID)  # [batch_size, patch_size]\n",
    "        global_input = torch.cat((global_padding, bytes[:, :-CONFIG.P]), dim=-1)  # [batch_size, sequence_length]\n",
    "        \n",
    "        bytes_input = rearrange(bytes, \"b (k p) -> (b k) p\", p=CONFIG.P)  # [batch_size * num_patches, patch_size]\n",
    "        local_padding = bytes_input.new(bytes_input.shape[0], 1).fill_(CONFIG.PAD_ID)  # [patch_size]\n",
    "        local_input = torch.cat((local_padding, bytes_input[:, :-1]), dim=-1)  # [batch_size * num_patches, patch_size]\n",
    "        return global_input, local_input\n",
    "    \n",
    "    def loss(self, bytes, y):  # y: [batch_size, sequence_length]\n",
    "        y = rearrange(y, \"b t -> (b t)\")  # [batch_size * sequence_length]\n",
    "        logits = self.forward(bytes)  # [batch_size, sequence_length, vocab_size]\n",
    "        logits = rearrange(logits, \"b t v -> (b t) v\", v=CONFIG.V)  # [batch_size * sequence_length, vocab_size]\n",
    "        return F.cross_entropy(logits, y, ignore_index=CONFIG.PAD_ID)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, bytes, max_len=None):\n",
    "        self.eval()\n",
    "        if max_len is None:\n",
    "            max_len = CONFIG.max_len\n",
    "        generated = bytes\n",
    "        for _ in range(max_len):\n",
    "            logits = self.forward(bytes)  # [batch_size, sequence_length, vocab_size]\n",
    "            logits = logits[:, -1, :]  # [batch_size, vocab_size], get the last token\n",
    "            _, next_token = torch.max(logits, dim=-1)  # [batch_size]\n",
    "            generated = torch.cat((generated, next_token.unsqueeze(-1)), dim=-1)  # [batch_size, sequence_length]\n",
    "        return generated"
   ],
   "id": "625b775dda52818a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:46.832016Z",
     "start_time": "2024-10-08T16:45:46.801209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "megabyte = MEGABYTE().to(CONFIG.device)\n",
    "print(megabyte)"
   ],
   "id": "4b7a8d3a9155e231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGABYTE(\n",
      "  (global_model): GlobalModel(\n",
      "    (patch_embedder): PatchEmbedder(\n",
      "      (embedding): Embedding(512, 256)\n",
      "      (positional_embedding): Embedding(1024, 256)\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-15): 16 x Head(\n",
      "            (query): Linear(in_features=1024, out_features=16, bias=True)\n",
      "            (key): Linear(in_features=1024, out_features=16, bias=True)\n",
      "            (value): Linear(in_features=1024, out_features=16, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (local_model): LocalModel(\n",
      "    (embedding): Embedding(512, 128)\n",
      "    (local_transformer): Decoder(\n",
      "      (attention): MultiHeadAttention(\n",
      "        (heads): ModuleList(\n",
      "          (0-7): 8 x Head(\n",
      "            (query): Linear(in_features=128, out_features=16, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=16, bias=True)\n",
      "            (value): Linear(in_features=128, out_features=16, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "        (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=128, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T16:45:47.625809Z",
     "start_time": "2024-10-08T16:45:46.832999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = megabyte.loss(sample_x, sample_y)\n",
    "print(loss)\n",
    "\n",
    "generated = megabyte.generate(sample_x)\n",
    "print(decode(generated[0].tolist()[0]))"
   ],
   "id": "a2e9cd2168cc8fa7",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m loss \u001B[38;5;241m=\u001B[39m megabyte\u001B[38;5;241m.\u001B[39mloss(sample_x, sample_y)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[1;32m      4\u001B[0m generated \u001B[38;5;241m=\u001B[39m megabyte\u001B[38;5;241m.\u001B[39mgenerate(sample_x)\n",
      "Cell \u001B[0;32mIn[24], line 24\u001B[0m, in \u001B[0;36mMEGABYTE.loss\u001B[0;34m(self, bytes, y)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mloss\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mbytes\u001B[39m, y):  \u001B[38;5;66;03m# y: [batch_size, sequence_length]\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     y \u001B[38;5;241m=\u001B[39m rearrange(y, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb t -> (b t)\u001B[39m\u001B[38;5;124m\"\u001B[39m)  \u001B[38;5;66;03m# [batch_size * sequence_length]\u001B[39;00m\n\u001B[0;32m---> 24\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;28mbytes\u001B[39m)  \u001B[38;5;66;03m# [batch_size, sequence_length, vocab_size]\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     logits \u001B[38;5;241m=\u001B[39m rearrange(logits, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb t v -> (b t) v\u001B[39m\u001B[38;5;124m\"\u001B[39m, v\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mV)  \u001B[38;5;66;03m# [batch_size * sequence_length, vocab_size]\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, y, ignore_index\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mPAD_ID)\n",
      "Cell \u001B[0;32mIn[24], line 9\u001B[0m, in \u001B[0;36mMEGABYTE.forward\u001B[0;34m(self, bytes)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28mbytes\u001B[39m):  \u001B[38;5;66;03m# [batch_size, sequence_length]\u001B[39;00m\n\u001B[1;32m      8\u001B[0m     global_input, local_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_input(\u001B[38;5;28mbytes\u001B[39m)  \u001B[38;5;66;03m# [batch_size, sequence_length], [batch_size * num_patches, patch_size]\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m     global_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_model(global_input)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[1;32m     10\u001B[0m     local_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_model(local_input, global_output)  \u001B[38;5;66;03m# [batch_size, sequence_length, vocab_size]\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m local_output\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[22], line 11\u001B[0m, in \u001B[0;36mGlobalModel.forward\u001B[0;34m(self, bytes)\u001B[0m\n\u001B[1;32m      9\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch_embedder(\u001B[38;5;28mbytes\u001B[39m)  \u001B[38;5;66;03m# [batch_size, num_patches, patch_size * d_embed]\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(CONFIG\u001B[38;5;241m.\u001B[39mn_layers_G):\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(x)  \u001B[38;5;66;03m# [batch_size, num_patches, patch_size * d_embed]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m x \u001B[38;5;241m=\u001B[39m rearrange(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb k (p d) -> (b k) p d\u001B[39m\u001B[38;5;124m\"\u001B[39m, b\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbytes\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], k\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mK, p\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mP, d\u001B[38;5;241m=\u001B[39mCONFIG\u001B[38;5;241m.\u001B[39mD_G)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, d_embed]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[20], line 11\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):  \u001B[38;5;66;03m# [batch_size, block_size, d_embed]\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x))  \u001B[38;5;66;03m# [batch_size, block_size, d_embed]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x))  \u001B[38;5;66;03m# [batch_size, block_size, vocab_size]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (1024) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "7edb7b78f1714e9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model):\n",
    "    model = model.to(CONFIG.device)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    for epoch in range(CONFIG.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CONFIG.epochs}'):\n",
    "            x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss.append(running_loss / len(train_loader))\n",
    "        print(f'Training Loss: {running_loss / len(train_loader)}')\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "                loss = model.loss(x, y)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "        validation_loss.append(running_loss / len(validation_loader))\n",
    "        print(f'Validation Loss: {running_loss / len(validation_loader)}')\n",
    "        \n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "1b495cc61a877c49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train(megabyte)",
   "id": "3d97d7626c626591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e70edc36e643819e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "generated = megabyte.generate(sample_x)\n",
    "print(decode(generated[0].tolist()))"
   ],
   "id": "c5067e98aebbc674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "495822436e557079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "acd82062c9902c0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

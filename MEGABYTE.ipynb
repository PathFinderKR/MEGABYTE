{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "52896242a346d463"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.171899Z",
     "start_time": "2024-10-10T10:23:49.214352Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "id": "d389d6b8bf136b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.175163Z",
     "start_time": "2024-10-10T10:23:50.173038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch size = P\n",
    "# context size = T\n",
    "# Number of patches = K = T/P\n",
    "# global embedding dimension = d_G\n",
    "# local embedding dimension = d_L"
   ],
   "id": "c4ef3d46e76a5317",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.183301Z",
     "start_time": "2024-10-10T10:23:50.176042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    V: int = 512  # 258 utf-8 characters + 2 special tokens\n",
    "    P: int = 4\n",
    "    T: int = 1024\n",
    "    K: int = T // P  # Number of patches\n",
    "    \n",
    "    model_size: str = 'small'  # 'small' or 'large'\n",
    "    \n",
    "    ## Small\n",
    "    if model_size == 'small':\n",
    "        ### Global model\n",
    "        n_layers_G: int = 6\n",
    "        n_heads_G: int = 4\n",
    "        d_G: int = 128\n",
    "        d_head_G: int = d_G // n_heads_G\n",
    "        d_ff_G: int = d_G * 4\n",
    "        dropout_G: float = 0.1\n",
    "        ### Local model\n",
    "        n_layers_L: int = 4\n",
    "        n_heads_L: int = 4\n",
    "        d_L: int = 64\n",
    "        d_head_L: int = d_L // n_heads_L\n",
    "        d_ff_L: int = d_L * 4\n",
    "        dropout_L: float = 0.1\n",
    "    ### Large\n",
    "    elif model_size == 'large':\n",
    "        ### Global model\n",
    "        n_layers_G: int = 12\n",
    "        n_heads_G: int = 8\n",
    "        d_G: int = 256\n",
    "        d_head_G: int = d_G // n_heads_G\n",
    "        d_ff_G: int = d_G * 4\n",
    "        dropout_G: float = 0.1\n",
    "        ### Local model\n",
    "        n_layers_L: int = 8\n",
    "        n_heads_L: int = 8\n",
    "        d_L: int = 128\n",
    "        d_head_L: int = d_L // n_heads_L\n",
    "        d_ff_L: int = d_L * 4\n",
    "        dropout_L: float = 0.2\n",
    "    \n",
    "    flash_attention: bool = False\n",
    "    \n",
    "    # Vocabulary\n",
    "    PAD_ID: int = 256\n",
    "    EOS_ID: int = 257\n",
    "    \n",
    "    # data\n",
    "    validation_size: float = 0.2\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 128\n",
    "    learning_rate: float = 2e-5\n",
    "    \n",
    "    # Generation\n",
    "    max_len: int = 8192\n",
    "    temperature: float = 1.0\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 101"
   ],
   "id": "60e93c2e04de8080",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "13c03a9d7d545aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.196072Z",
     "start_time": "2024-10-10T10:23:50.184428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "fce8043802833d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 101\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "aef10a529a5447d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.496391Z",
     "start_time": "2024-10-10T10:23:50.196894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "39f348183c5da647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on GPU | Num of GPUs:  1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug",
   "id": "a405beab399da1ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.499663Z",
     "start_time": "2024-10-10T10:23:50.497455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.epochs = 1\n",
    "    CONFIG.max_len = 2000"
   ],
   "id": "c18def8c7828e0ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "fe4f42ed478346e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.507035Z",
     "start_time": "2024-10-10T10:23:50.500502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data path\n",
    "dataset_path = 'data/'\n",
    "# shakespeare data\n",
    "shakespeare_dataset = dataset_path + 'shakespeare.txt'"
   ],
   "id": "74c5f6ded01e7533",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.518835Z",
     "start_time": "2024-10-10T10:23:50.507966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the dataset\n",
    "with open(shakespeare_dataset, 'r', encoding='utf-8') as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "id": "31b5b3b8de611ece",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.522374Z",
     "start_time": "2024-10-10T10:23:50.519849Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:1000])",
   "id": "4e9a6798598cc0d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.529359Z",
     "start_time": "2024-10-10T10:23:50.524054Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Total number of characters in the text: {len(shakespeare_text)}')",
   "id": "e851ba99cbf035c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 1115394\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vocabulary",
   "id": "505c578a5ddc11b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.536159Z",
     "start_time": "2024-10-10T10:23:50.530142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char2int = {chr(i): i for i in range(CONFIG.V)}\n",
    "int2char = {i: chr(i) for i in range(CONFIG.V)}\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "char2int[PAD_TOKEN] = CONFIG.PAD_ID\n",
    "int2char[CONFIG.PAD_ID] = PAD_TOKEN\n",
    "EOS_TOKEN = '<EOS>'\n",
    "char2int[EOS_TOKEN] = CONFIG.EOS_ID\n",
    "int2char[CONFIG.EOS_ID] = EOS_TOKEN"
   ],
   "id": "6758e282bc77c5bd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.542982Z",
     "start_time": "2024-10-10T10:23:50.537053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(text):\n",
    "    return [char2int[c] for c in text]\n",
    "\n",
    "def decode(tokens):\n",
    "    return ''.join([int2char[t] for t in tokens])"
   ],
   "id": "38c161e7f325023e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.550661Z",
     "start_time": "2024-10-10T10:23:50.543891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = \"Hello, World!\" + EOS_TOKEN\n",
    "sample_tokens = encode(sample_text)\n",
    "print(sample_tokens)\n",
    "print(decode(sample_tokens))"
   ],
   "id": "fb83b51bf61a3dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33, 60, 69, 79, 83, 62]\n",
      "Hello, World!<EOS>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.644080Z",
     "start_time": "2024-10-10T10:23:50.551550Z"
    }
   },
   "cell_type": "code",
   "source": "shakespeare_tokens = torch.tensor(encode(shakespeare_text))",
   "id": "c2f9b47e50c0575b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "a16efac757bb6828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.648009Z",
     "start_time": "2024-10-10T10:23:50.645046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Validation Split\n",
    "train_size = int(len(shakespeare_tokens) * (1 - CONFIG.validation_size))\n",
    "train_tokens = shakespeare_tokens[:train_size]\n",
    "validation_tokens = shakespeare_tokens[train_size:]\n",
    "print(f'Total number of tokens in the training set: {len(train_tokens)}')\n",
    "print(f'Total number of tokens in the validation set: {len(validation_tokens)}')"
   ],
   "id": "2d8b1d2c86b75d06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the training set: 892315\n",
      "Total number of tokens in the validation set: 223079\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.655132Z",
     "start_time": "2024-10-10T10:23:50.648817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = tokens\n",
    "        self.context_length= context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.context_length], self.tokens[idx+1:idx+self.context_length+1]\n",
    "\n",
    "train_dataset = ShakespeareDataset(train_tokens, CONFIG.T)\n",
    "validation_dataset = ShakespeareDataset(validation_tokens, CONFIG.T)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.batch_size, shuffle=False)"
   ],
   "id": "10bccc9360cffba3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.839977Z",
     "start_time": "2024-10-10T10:23:50.655987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x, sample_y = sample_x.to(CONFIG.device), sample_y.to(CONFIG.device)\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(sample_x[0])\n",
    "print(sample_y[0])"
   ],
   "id": "36aefea017f1c78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1024]) torch.Size([128, 1024])\n",
      "tensor([101, 110, 115,  ...,  97, 110,  32], device='cuda:0')\n",
      "tensor([110, 115, 107,  ..., 110,  32, 119], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c45df9ae58cabd96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "36e603385ec68491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.845699Z",
     "start_time": "2024-10-10T10:23:50.841194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_heads: int, d_head: int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.query = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.key = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.value = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.out = nn.Linear(self.n_heads * self.d_head, self.d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        batch_size, context_size, _ = x.size()\n",
    "        q = self.query(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        k = self.key(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        v = self.value(x).view(batch_size, context_size, self.n_heads, self.d_head)  # [batch_size, context_size, n_heads, d_head]\n",
    "        \n",
    "        q = q.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        k = k.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        v = v.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        \n",
    "        # Masked Self Attention\n",
    "        mask = torch.triu(torch.ones(context_size, context_size, device=x.device), diagonal=1).bool()  # [context_size, context_size]\n",
    "        mask = mask.view(1, 1, context_size, context_size)  # [1, 1, context_size, context_size]\n",
    "        mask = mask.repeat(batch_size, self.n_heads, 1, 1)  # [batch_size, n_heads, context_size, context_size]\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)  # [batch_size, n_heads, context_size, context_size]\n",
    "        scores = scores.masked_fill(mask, float('-inf'))  # [batch_size, n_heads, context_size, context_size]\n",
    "        scores = F.softmax(scores, dim=-1)  # [batch_size, n_heads, context_size, context_size]\n",
    "        \n",
    "        x = torch.matmul(scores, v)  # [batch_size, n_heads, context_size, d_head]\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, context_size, self.n_heads * self.d_head)  # [batch_size, context_size, n_heads * d_head]\n",
    "        x = self.out(x)  # [batch_size, context_size, d_embed]\n",
    "        return x"
   ],
   "id": "2c4f95af6a999a2a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.854110Z",
     "start_time": "2024-10-10T10:23:50.846570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_embed, d_ff, bias=False)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        x = F.gelu(self.fc1(x))  # [batch_size, context_size, d_ff]\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)  # [batch_size, context_size, d_embed]\n",
    "        return x"
   ],
   "id": "623ef725d6ec7186",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.861089Z",
     "start_time": "2024-10-10T10:23:50.855097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_head: int, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = CausalSelfAttention(d_embed=d_embed, n_heads=n_heads, d_head=d_head)\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "        self.mlp = MLP(d_embed=d_embed, d_ff=d_ff, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.attention(self.norm1(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.mlp(self.norm2(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "2ca7331d305226e7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MEGABYTE",
   "id": "ca7d1e714568e5cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.869967Z",
     "start_time": "2024-10-10T10:23:50.862041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_G)\n",
    "        self.positional_embedding = nn.Embedding(config.T, config.d_G)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        assert self.config.T % self.config.P == 0, \"context size must be divisible by patch size\"\n",
    "        \n",
    "        bytes = self.embedding(bytes) + self.positional_embedding(torch.arange(self.config.T, device=bytes.device))  # [batch_size, context_size, d_embed]\n",
    "        bytes = rearrange(bytes, \"b (k p) d -> b k (p d)\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        return bytes"
   ],
   "id": "aca577b5d80002f1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.877017Z",
     "start_time": "2024-10-10T10:23:50.870843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedder = PatchEmbedder(config)\n",
    "        self.decoder = Decoder(n_heads=config.n_heads_G, d_head=config.d_head_G, d_embed=config.P*config.d_G, d_ff=config.d_ff_G, dropout=config.dropout_G)\n",
    "        self.linear = nn.Linear(config.d_G, config.d_L, bias=False)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        x = self.patch_embedder(bytes)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        for _ in range(self.config.n_layers_G):\n",
    "            x = self.decoder(x)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        x = rearrange(x, \"b k (p d) -> (b k) p d\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size * num_patches, patch_size, d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "4962881ec29397b7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.885705Z",
     "start_time": "2024-10-10T10:23:50.877982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_L)\n",
    "        self.local_transformer = Decoder(n_heads=config.n_heads_L, d_head=config.d_head_L, d_embed=config.d_L, d_ff=config.d_ff_L, dropout=config.dropout_L)\n",
    "        self.linear = nn.Linear(config.d_L, config.V, bias=False)\n",
    "        \n",
    "    def forward(self, local_input, global_output):  # [batch_size * num_patches, patch_size], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.embedding(local_input) + global_output  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        for _ in range(self.config.n_layers_L):\n",
    "            x = self.local_transformer(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, vocab_size]\n",
    "        x = rearrange(x, \"(b k) p v -> b (k p) v\", k=self.config.K, p=self.config.P, v=self.config.V)  # [batch_size, context_size, vocab_size]\n",
    "        return x"
   ],
   "id": "98d05f3337de7d5b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.893419Z",
     "start_time": "2024-10-10T10:23:50.886781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MEGABYTE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.global_model = GlobalModel(config)\n",
    "        self.local_model = LocalModel(config)\n",
    "        self.max_len = config.max_len\n",
    "        self.context_size = config.T\n",
    "        self.temperature = config.temperature\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        global_input, local_input = self.prepare_input(bytes)  # [batch_size, context_size], [batch_size * num_patches, patch_size]\n",
    "        global_output = self.global_model(global_input)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        local_output = self.local_model(local_input, global_output)  # [batch_size, context_size, vocab_size]\n",
    "        return local_output\n",
    "        \n",
    "    def prepare_input(self, bytes):  # [batch_size, context_size]\n",
    "        global_padding = bytes.new(bytes.shape[0], self.config.P).fill_(self.config.PAD_ID)  # [batch_size, patch_size]\n",
    "        global_input = torch.cat((global_padding, bytes[:, :-CONFIG.P]), dim=-1)  # [batch_size, context_size]\n",
    "        \n",
    "        bytes_input = rearrange(bytes, \"b (k p) -> (b k) p\", p=self.config.P)  # [batch_size * num_patches, patch_size]\n",
    "        local_padding = bytes_input.new(bytes_input.shape[0], 1).fill_(self.config.PAD_ID)  # [patch_size]\n",
    "        local_input = torch.cat((local_padding, bytes_input[:, :-1]), dim=-1)  # [batch_size * num_patches, patch_size]\n",
    "        return global_input, local_input\n",
    "    \n",
    "    def loss(self, bytes, y):  # y: [batch_size, context_size]\n",
    "        y = rearrange(y, \"b t -> (b t)\")  # [batch_size * context_size]\n",
    "        logits = self.forward(bytes)  # [batch_size, context_size, vocab_size]\n",
    "        logits = rearrange(logits, \"b t v -> (b t) v\", v=self.config.V)  # [batch_size * context_size, vocab_size]\n",
    "        return F.cross_entropy(logits, y, ignore_index=self.config.PAD_ID)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, max_len=None, decode_fn=None):\n",
    "        self.eval()\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "    \n",
    "        for _ in range(max_len - x.size(1)):  # x: [batch_size, context]\n",
    "            context = x[:, -self.context_size:]  # [batch_size, context_size]\n",
    "            output = self.forward(context)  # [batch_size, context_size, vocab_size]\n",
    "            logits = output[:, -1, :] / self.temperature\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(-1)  # [batch_size]\n",
    "            x = torch.cat((x, next_token.unsqueeze(-1)), dim=-1)  # [batch_size, context]\n",
    "    \n",
    "            # Decode token\n",
    "            if decode_fn is not None:\n",
    "                decoded_token = decode_fn([next_token[0].item()])\n",
    "                print(decoded_token, end='', flush=True)\n",
    "                \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ],
   "id": "625b775dda52818a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T10:23:50.910612Z",
     "start_time": "2024-10-10T10:23:50.894480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "megabyte = MEGABYTE(CONFIG).to(CONFIG.device)\n",
    "print(megabyte)\n"
   ],
   "id": "4b7a8d3a9155e231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGABYTE(\n",
      "  (global_model): GlobalModel(\n",
      "    (patch_embedder): PatchEmbedder(\n",
      "      (embedding): Embedding(512, 128)\n",
      "      (positional_embedding): Embedding(1024, 128)\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (key): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (value): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (out): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=False)\n",
      "  )\n",
      "  (local_model): LocalModel(\n",
      "    (embedding): Embedding(512, 64)\n",
      "    (local_transformer): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out): Linear(in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-10T10:23:50.911555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = megabyte.loss(sample_x, sample_y)\n",
    "print(loss)\n",
    "\n",
    "megabyte.generate(sample_x, max_len=2048, decode_fn=decode)"
   ],
   "id": "a2e9cd2168cc8fa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.9197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "4Ç¸Ǿ¾ÉŁ÷ç\u001D2ÏƆųƿŶƿÊmàƆƻĵ\u0006ąǒ\u0005Űƿ^èƨ\u0002ivÝǽĐóġǂǷǙōÞxźîñęǖİęaǠǒƣǳÚÞ\u0010SûäRŘǩƌƿóĚŗû^ƭŁ\u001DxĜ¢Ĥ{ÍƸçǛĽƁǄÍæĬóZĲmġºĉÅ"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "7edb7b78f1714e9a"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model):\n",
    "    model = model.to(CONFIG.device)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    for epoch in range(CONFIG.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CONFIG.epochs}'):\n",
    "            x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss.append(running_loss / len(train_loader))\n",
    "        print(f'Training Loss: {running_loss / len(train_loader)}')\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "                loss = model.loss(x, y)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "        validation_loss.append(running_loss / len(validation_loader))\n",
    "        print(f'Validation Loss: {running_loss / len(validation_loader)}')\n",
    "        \n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "1b495cc61a877c49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "train(megabyte)",
   "id": "3d97d7626c626591",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e70edc36e643819e"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "megabyte.generate(sample_x, decode_fn=decode)",
   "id": "c5067e98aebbc674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "495822436e557079"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "acd82062c9902c0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

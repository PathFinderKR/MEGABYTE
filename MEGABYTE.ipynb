{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Importing Libraries",
   "id": "52896242a346d463"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.924262Z",
     "start_time": "2024-10-10T05:51:45.111336Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameters",
   "id": "d389d6b8bf136b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.927857Z",
     "start_time": "2024-10-10T05:51:45.926121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Patch size = P\n",
    "# context size = T\n",
    "# Number of patches = K = T/P\n",
    "# global embedding dimension = d_G\n",
    "# local embedding dimension = d_L"
   ],
   "id": "c4ef3d46e76a5317",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.931807Z",
     "start_time": "2024-10-10T05:51:45.928329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    debug: bool = False\n",
    "    \n",
    "    # Model\n",
    "    V: int = 512  # 258 utf-8 characters + 2 special tokens\n",
    "    P: int = 4\n",
    "    T: int = 1024\n",
    "    K: int = T // P  # Number of patches\n",
    "    \n",
    "    ## Global model\n",
    "    n_layers_G: int = 8\n",
    "    n_heads_G: int = 4\n",
    "    d_G: int = 128\n",
    "    d_head_G: int = d_G // n_heads_G\n",
    "    d_ff_G: int = d_G * 4\n",
    "    dropout_G: float = 0.1\n",
    "    \n",
    "    ## Local model\n",
    "    n_layers_L: int = 4\n",
    "    n_heads_L: int = 2\n",
    "    d_L: int = 64\n",
    "    d_head_L: int = d_L // n_heads_L\n",
    "    d_ff_L: int = d_L * 4\n",
    "    dropout_L: float = 0.1\n",
    "    \n",
    "    flash_attention: bool = False\n",
    "    \n",
    "    # Vocabulary\n",
    "    PAD_ID: int = 256\n",
    "    EOS_ID: int = 257\n",
    "    \n",
    "    # data\n",
    "    validation_size: float = 0.2\n",
    "    \n",
    "    # Device\n",
    "    device: torch.device = None\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 2\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    \n",
    "    # Generation\n",
    "    max_len: int = 8192\n",
    "    \n",
    "    # Seed\n",
    "    seed: int = 101"
   ],
   "id": "60e93c2e04de8080",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Reproducibility",
   "id": "13c03a9d7d545aa5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.935673Z",
     "start_time": "2024-10-10T05:51:45.932956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    print(f\"Seed: {seed}\")\n",
    "    \n",
    "set_seed(CONFIG.seed)"
   ],
   "id": "fce8043802833d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 101\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Device",
   "id": "aef10a529a5447d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.946706Z",
     "start_time": "2024-10-10T05:51:45.936133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def configure_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(\"> Running on GPU\", end=' | ')\n",
    "        print(\"Num of GPUs: \", num_gpu)\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"> Running on MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"> Running on CPU\")\n",
    "    return device\n",
    "\n",
    "CONFIG.device = configure_device()"
   ],
   "id": "39f348183c5da647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running on MPS\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Debug",
   "id": "a405beab399da1ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.948452Z",
     "start_time": "2024-10-10T05:51:45.947205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if CONFIG.debug:\n",
    "    CONFIG.epochs = 1\n",
    "    CONFIG.max_len = 2000"
   ],
   "id": "c18def8c7828e0ef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "fe4f42ed478346e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.950463Z",
     "start_time": "2024-10-10T05:51:45.948829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# data path\n",
    "dataset_path = 'data/'\n",
    "# shakespeare data\n",
    "shakespeare_dataset = dataset_path + 'shakespeare.txt'"
   ],
   "id": "74c5f6ded01e7533",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.952701Z",
     "start_time": "2024-10-10T05:51:45.950967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read the dataset\n",
    "with open(shakespeare_dataset, 'r', encoding='utf-8') as f:\n",
    "    shakespeare_text = f.read()"
   ],
   "id": "31b5b3b8de611ece",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.954411Z",
     "start_time": "2024-10-10T05:51:45.953097Z"
    }
   },
   "cell_type": "code",
   "source": "print(shakespeare_text[:1000])",
   "id": "4e9a6798598cc0d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.957136Z",
     "start_time": "2024-10-10T05:51:45.955836Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Total number of characters in the text: {len(shakespeare_text)}')",
   "id": "e851ba99cbf035c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the text: 1115394\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Vocabulary",
   "id": "505c578a5ddc11b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.959620Z",
     "start_time": "2024-10-10T05:51:45.957537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "char2int = {chr(i): i for i in range(CONFIG.V)}\n",
    "int2char = {i: chr(i) for i in range(CONFIG.V)}\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "char2int[PAD_TOKEN] = CONFIG.PAD_ID\n",
    "int2char[CONFIG.PAD_ID] = PAD_TOKEN\n",
    "EOS_TOKEN = '<EOS>'\n",
    "char2int[EOS_TOKEN] = CONFIG.EOS_ID\n",
    "int2char[CONFIG.EOS_ID] = EOS_TOKEN"
   ],
   "id": "6758e282bc77c5bd",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.961399Z",
     "start_time": "2024-10-10T05:51:45.960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode(text):\n",
    "    return [char2int[c] for c in text]\n",
    "\n",
    "def decode(tokens):\n",
    "    return ''.join([int2char[t] for t in tokens])"
   ],
   "id": "38c161e7f325023e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:45.963418Z",
     "start_time": "2024-10-10T05:51:45.961755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_text = \"Hello, World!\" + EOS_TOKEN\n",
    "sample_tokens = encode(sample_text)\n",
    "print(sample_tokens)\n",
    "print(decode(sample_tokens))"
   ],
   "id": "fb83b51bf61a3dba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33, 60, 69, 79, 83, 62]\n",
      "Hello, World!<EOS>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.048572Z",
     "start_time": "2024-10-10T05:51:45.963829Z"
    }
   },
   "cell_type": "code",
   "source": "shakespeare_tokens = torch.tensor(encode(shakespeare_text))",
   "id": "c2f9b47e50c0575b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocessing",
   "id": "a16efac757bb6828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.050870Z",
     "start_time": "2024-10-10T05:51:46.049160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Validation Split\n",
    "train_size = int(len(shakespeare_tokens) * (1 - CONFIG.validation_size))\n",
    "train_tokens = shakespeare_tokens[:train_size]\n",
    "validation_tokens = shakespeare_tokens[train_size:]\n",
    "print(f'Total number of tokens in the training set: {len(train_tokens)}')\n",
    "print(f'Total number of tokens in the validation set: {len(validation_tokens)}')"
   ],
   "id": "2d8b1d2c86b75d06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in the training set: 892315\n",
      "Total number of tokens in the validation set: 223079\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.053594Z",
     "start_time": "2024-10-10T05:51:46.051439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = tokens\n",
    "        self.context_length= context_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.context_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokens[idx:idx+self.context_length], self.tokens[idx+1:idx+self.context_length+1]\n",
    "\n",
    "train_dataset = ShakespeareDataset(train_tokens, CONFIG.T)\n",
    "validation_dataset = ShakespeareDataset(validation_tokens, CONFIG.T)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=CONFIG.batch_size, shuffle=False)"
   ],
   "id": "10bccc9360cffba3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.115305Z",
     "start_time": "2024-10-10T05:51:46.054018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_x, sample_y = next(iter(train_loader))\n",
    "sample_x, sample_y = sample_x.to(CONFIG.device), sample_y.to(CONFIG.device)\n",
    "print(sample_x.shape, sample_y.shape)\n",
    "print(sample_x[0])\n",
    "print(sample_y[0])"
   ],
   "id": "36aefea017f1c78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024]) torch.Size([16, 1024])\n",
      "tensor([101, 110, 115,  ...,  97, 110,  32], device='mps:0')\n",
      "tensor([110, 115, 107,  ..., 110,  32, 119], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "c45df9ae58cabd96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transformer",
   "id": "36e603385ec68491"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.118491Z",
     "start_time": "2024-10-10T05:51:46.115861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_heads: int, d_head: int):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_head\n",
    "        \n",
    "        self.query = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.key = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.value = nn.Linear(self.d_embed, self.n_heads * self.d_head, bias=False)\n",
    "        self.out = nn.Linear(self.n_heads * self.d_head, self.d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        batch_size, context_size, _ = x.size()\n",
    "        q = self.query(x).view(batch_size, context_size, self.n_heads, self.d_head)\n",
    "        k = self.key(x).view(batch_size, context_size, self.n_heads, self.d_head)\n",
    "        v = self.value(x).view(batch_size, context_size, self.n_heads, self.d_head)\n",
    "        \n",
    "        q = q.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        k = k.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        v = v.transpose(1, 2)  # [batch_size, n_heads, context_size, d_head]\n",
    "        \n",
    "        attention = F.softmax(torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(self.d_head), dim=-1)  # [batch_size, n_heads, context_size, context_size]\n",
    "        x = torch.matmul(attention, v)  # [batch_size, n_heads, context_size, d_head]\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, context_size, self.n_heads * self.d_head)\n",
    "        x = self.out(x)\n",
    "        return x"
   ],
   "id": "2c4f95af6a999a2a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.120868Z",
     "start_time": "2024-10-10T05:51:46.118960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_embed, d_ff, bias=False)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed, bias=False)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed]\n",
    "        x = F.gelu(self.fc1(x))  # [batch_size, context_size, d_ff]\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.fc2(x)  # [batch_size, context_size, d_embed]\n",
    "        return x"
   ],
   "id": "623ef725d6ec7186",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.123218Z",
     "start_time": "2024-10-10T05:51:46.121404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_heads: int, d_head: int, d_embed: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.attention = CausalSelfAttention(d_embed=d_embed, n_heads=n_heads, d_head=d_head)\n",
    "        self.norm1 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "        self.mlp = MLP(d_embed=d_embed, d_ff=d_ff, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(d_embed)\n",
    "        \n",
    "    def forward(self, x):  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.attention(self.norm1(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = x + self.mlp(self.norm2(x))  # [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "2ca7331d305226e7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MEGABYTE",
   "id": "ca7d1e714568e5cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.125746Z",
     "start_time": "2024-10-10T05:51:46.123673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PatchEmbedder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_G)\n",
    "        self.positional_embedding = nn.Embedding(config.T, config.d_G)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        assert self.config.T % self.config.P == 0, \"context size must be divisible by patch size\"\n",
    "        \n",
    "        bytes = self.embedding(bytes) + self.positional_embedding(torch.arange(self.config.T, device=bytes.device))  # [batch_size, context_size, d_embed]\n",
    "        bytes = rearrange(bytes, \"b (k p) d -> b k (p d)\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        return bytes"
   ],
   "id": "aca577b5d80002f1",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.128185Z",
     "start_time": "2024-10-10T05:51:46.126268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GlobalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embedder = PatchEmbedder(config)\n",
    "        self.decoder = Decoder(n_heads=config.n_heads_G, d_head=config.d_head_G, d_embed=config.P*config.d_G, d_ff=config.d_ff_G, dropout=config.dropout_G)\n",
    "        self.linear = nn.Linear(config.d_G, config.d_L, bias=False)\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        x = self.patch_embedder(bytes)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        for _ in range(self.config.n_layers_G):\n",
    "            x = self.decoder(x)  # [batch_size, num_patches, patch_size * d_embed]\n",
    "        x = rearrange(x, \"b k (p d) -> (b k) p d\", b=bytes.shape[0], k=self.config.K, p=self.config.P, d=self.config.d_G)  # [batch_size * num_patches, patch_size, d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        return x"
   ],
   "id": "4962881ec29397b7",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.130637Z",
     "start_time": "2024-10-10T05:51:46.128772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocalModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(config.V, config.d_L)\n",
    "        self.local_transformer = Decoder(n_heads=config.n_heads_L, d_head=config.d_head_L, d_embed=config.d_L, d_ff=config.d_ff_L, dropout=config.dropout_L)\n",
    "        self.linear = nn.Linear(config.d_L, config.V, bias=False)\n",
    "        \n",
    "    def forward(self, local_input, global_output):  # [batch_size * num_patches, patch_size], [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.embedding(local_input) + global_output  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        for _ in range(self.config.n_layers_L):\n",
    "            x = self.local_transformer(x)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        x = self.linear(x)  # [batch_size * num_patches, patch_size, vocab_size]\n",
    "        x = rearrange(x, \"(b k) p v -> b (k p) v\", k=self.config.K, p=self.config.P, v=self.config.V)  # [batch_size, context_size, vocab_size]\n",
    "        return x"
   ],
   "id": "98d05f3337de7d5b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.134387Z",
     "start_time": "2024-10-10T05:51:46.131037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MEGABYTE(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.global_model = GlobalModel(config)\n",
    "        self.local_model = LocalModel(config)\n",
    "        self.max_len = config.max_len\n",
    "        self.context_size = config.T\n",
    "        \n",
    "    def forward(self, bytes):  # [batch_size, context_size]\n",
    "        global_input, local_input = self.prepare_input(bytes)  # [batch_size, context_size], [batch_size * num_patches, patch_size]\n",
    "        global_output = self.global_model(global_input)  # [batch_size * num_patches, patch_size, local_d_embed]\n",
    "        local_output = self.local_model(local_input, global_output)  # [batch_size, context_size, vocab_size]\n",
    "        return local_output\n",
    "        \n",
    "    def prepare_input(self, bytes):  # [batch_size, context_size]\n",
    "        global_padding = bytes.new(bytes.shape[0], self.config.P).fill_(self.config.PAD_ID)  # [batch_size, patch_size]\n",
    "        global_input = torch.cat((global_padding, bytes[:, :-CONFIG.P]), dim=-1)  # [batch_size, context_size]\n",
    "        \n",
    "        bytes_input = rearrange(bytes, \"b (k p) -> (b k) p\", p=self.config.P)  # [batch_size * num_patches, patch_size]\n",
    "        local_padding = bytes_input.new(bytes_input.shape[0], 1).fill_(self.config.PAD_ID)  # [patch_size]\n",
    "        local_input = torch.cat((local_padding, bytes_input[:, :-1]), dim=-1)  # [batch_size * num_patches, patch_size]\n",
    "        return global_input, local_input\n",
    "    \n",
    "    def loss(self, bytes, y):  # y: [batch_size, context_size]\n",
    "        y = rearrange(y, \"b t -> (b t)\")  # [batch_size * context_size]\n",
    "        logits = self.forward(bytes)  # [batch_size, context_size, vocab_size]\n",
    "        logits = rearrange(logits, \"b t v -> (b t) v\", v=self.config.V)  # [batch_size * context_size, vocab_size]\n",
    "        return F.cross_entropy(logits, y, ignore_index=self.config.PAD_ID)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, x, max_len=None, decode_fn=None):\n",
    "        self.eval()\n",
    "        if max_len is None:\n",
    "            max_len = self.max_len\n",
    "    \n",
    "        for _ in range(max_len - x.size(1)):  # x: [batch_size, context]\n",
    "            context = x[:, -self.context_size:]  # [batch_size, context_size]\n",
    "            output = self.forward(context)  # [batch_size, context_size, vocab_size]\n",
    "            next_token = torch.argmax(output[:, -1, :], dim=-1)  # [batch_size, 1]\n",
    "            x = torch.cat((x, next_token.unsqueeze(-1)), dim=-1)  # [batch_size, context]\n",
    "            \n",
    "            # Decode token\n",
    "            if decode_fn is not None:\n",
    "                decoded_token = decode_fn([next_token[0].item()])\n",
    "                print(decoded_token, end='', flush=True)"
   ],
   "id": "625b775dda52818a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:51:46.148273Z",
     "start_time": "2024-10-10T05:51:46.134877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "megabyte = MEGABYTE(CONFIG).to(CONFIG.device)\n",
    "print(megabyte)"
   ],
   "id": "4b7a8d3a9155e231",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEGABYTE(\n",
      "  (global_model): GlobalModel(\n",
      "    (patch_embedder): PatchEmbedder(\n",
      "      (embedding): Embedding(512, 128)\n",
      "      (positional_embedding): Embedding(1024, 128)\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (key): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (value): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (out): Linear(in_features=128, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=512, out_features=512, bias=False)\n",
      "        (fc2): Linear(in_features=512, out_features=512, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=128, out_features=64, bias=False)\n",
      "  )\n",
      "  (local_model): LocalModel(\n",
      "    (embedding): Embedding(512, 64)\n",
      "    (local_transformer): Decoder(\n",
      "      (attention): CausalSelfAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out): Linear(in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=False)\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (linear): Linear(in_features=64, out_features=512, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:52:56.677283Z",
     "start_time": "2024-10-10T05:51:46.148993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss = megabyte.loss(sample_x, sample_y)\n",
    "print(loss)\n",
    "\n",
    "megabyte.generate(sample_x, decode_fn=decode)"
   ],
   "id": "a2e9cd2168cc8fa7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.8976, device='mps:0', grad_fn=<NllLossBackward0>)\n",
      "RęþęęŽęjęƗƣ­ǳǄÿǹŽġÏBǳġÞźBÏġǛBęġƆÜçǲÏÉǛÈęRęľőƼęÓęƗ{ǴƹęŃÞġæBÄƽǛŁęČǩę«ęÏjǲƗøíjjÞĦæŃŃǖŶƗBÈġäBęŽÞÄƙǄęMÞøÉÏŨǛÞVÏjÜǺƹïŃÏBƼġǬBęŽÞƗƙ­ƗZÞRęÏęęƗęíęŨęçÓąÜǊźÉVǳƗǳÈÞÏBÏġǛÉęƈÞjƭÞǳä\u001DVǩjBƗíÏŨŃŗġŁÜǔźBƗǳÏǳBÞÉƆÄġƗBÏŽÞęƙęŁǩçĞÓÏƗVƠƗBÏBÏŽǛƗƟíjjÞŊǺŃ{ġƹBǳÞÜƙ\u001DŁÉRƈ@ÉŽŃœƗVƗƗÈÏÙBġBBóŽÞęƨƣŁǩǷĞÏƣÏçŗçƆĽ[ęjŊƗƹíŃġÏǲÂÞRźÏƗƠƗBíÏBBŽġƗ¾jƗÞ­æZŃRġĽÜę\u001DÞRǳǳƣÞBęÞęƭęÜǩŁĹwBQġÞźMVÞäÉÏǬǳVÞƗÏĽÄRÈľRƉŞŽęÄęǳŁÞĲBÞġƭǳŽǩƗĹÏýƼÞőÉęÄęǳęÜŔÏÞ{æǷŃÉĽǬęƣęBęÏęǛw>íçÏÓVƗǖÏÞÄæǳÄǩøĞÏƗùÈƹRƗƣ\u0011ÄBǳę\u001DǩǊÓÉƗǅƗjŃƗƗR­BgwPÉjŊƹ>ŃÏƆBõÞÏƙŃŁġƈźƩƗÞƛǳRŃǩġĞmjÞƗź­Bgġ@BŽġjƗßÈƗ\u001D­ǺBÂÜÓŃƗġƗBÈġƗRǠƣRǩÉÏęǛwÜƈŽƺÝġX\u001DñÏÄÏǳBÞŽÏǊÄǩVƗøƛÏÏŃÞġæROǩçĹÓýƗę4ƗøĽÞ\u0011ęÜę\u001DęBęęÏęǲwîPa\u001DBñǙŞZïRjőßęäZęĎęķęÞęæęķǩŋĹęýƗęƚęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęęę"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[26], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m loss \u001B[38;5;241m=\u001B[39m megabyte\u001B[38;5;241m.\u001B[39mloss(sample_x, sample_y)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[0;32m----> 4\u001B[0m megabyte\u001B[38;5;241m.\u001B[39mgenerate(sample_x, decode_fn\u001B[38;5;241m=\u001B[39mdecode)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[24], line 39\u001B[0m, in \u001B[0;36mMEGABYTE.generate\u001B[0;34m(self, x, max_len, decode_fn)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(max_len \u001B[38;5;241m-\u001B[39m x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m)):  \u001B[38;5;66;03m# x: [batch_size, context]\u001B[39;00m\n\u001B[1;32m     38\u001B[0m     context \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_size:]  \u001B[38;5;66;03m# [batch_size, context_size]\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(context)  \u001B[38;5;66;03m# [batch_size, context_size, vocab_size]\u001B[39;00m\n\u001B[1;32m     40\u001B[0m     next_token \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39margmax(output[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [batch_size, 1]\u001B[39;00m\n\u001B[1;32m     41\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((x, next_token\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# [batch_size, context]\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[24], line 13\u001B[0m, in \u001B[0;36mMEGABYTE.forward\u001B[0;34m(self, bytes)\u001B[0m\n\u001B[1;32m     11\u001B[0m global_input, local_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_input(\u001B[38;5;28mbytes\u001B[39m)  \u001B[38;5;66;03m# [batch_size, context_size], [batch_size * num_patches, patch_size]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m global_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mglobal_model(global_input)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m local_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_model(local_input, global_output)  \u001B[38;5;66;03m# [batch_size, context_size, vocab_size]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m local_output\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[23], line 12\u001B[0m, in \u001B[0;36mLocalModel.forward\u001B[0;34m(self, local_input, global_output)\u001B[0m\n\u001B[1;32m     10\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding(local_input) \u001B[38;5;241m+\u001B[39m global_output  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mn_layers_L):\n\u001B[0;32m---> 12\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocal_transformer(x)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)  \u001B[38;5;66;03m# [batch_size * num_patches, patch_size, vocab_size]\u001B[39;00m\n\u001B[1;32m     14\u001B[0m x \u001B[38;5;241m=\u001B[39m rearrange(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(b k) p v -> b (k p) v\u001B[39m\u001B[38;5;124m\"\u001B[39m, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mK, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mP, v\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mV)  \u001B[38;5;66;03m# [batch_size, context_size, vocab_size]\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[20], line 11\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):  \u001B[38;5;66;03m# [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x))  \u001B[38;5;66;03m# [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[1;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmlp(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x))  \u001B[38;5;66;03m# [batch_size, context_size, d_embed], [batch_size, num_patches, patch_size * d_embed], [batch_size * num_patches, patch_size, local_d_embed]\u001B[39;00m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[18], line 16\u001B[0m, in \u001B[0;36mCausalSelfAttention.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     14\u001B[0m batch_size, context_size, _ \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m     15\u001B[0m q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mquery(x)\u001B[38;5;241m.\u001B[39mview(batch_size, context_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_head)\n\u001B[0;32m---> 16\u001B[0m k \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey(x)\u001B[38;5;241m.\u001B[39mview(batch_size, context_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_head)\n\u001B[1;32m     17\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalue(x)\u001B[38;5;241m.\u001B[39mview(batch_size, context_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_heads, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_head)\n\u001B[1;32m     19\u001B[0m q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)  \u001B[38;5;66;03m# [batch_size, n_heads, context_size, d_head]\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "7edb7b78f1714e9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:52:59.594682Z",
     "start_time": "2024-10-10T05:52:59.590035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model):\n",
    "    model = model.to(CONFIG.device)\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG.learning_rate)\n",
    "    \n",
    "    train_loss = []\n",
    "    validation_loss = []\n",
    "    \n",
    "    for epoch in range(CONFIG.epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{CONFIG.epochs}'):\n",
    "            x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        train_loss.append(running_loss / len(train_loader))\n",
    "        print(f'Training Loss: {running_loss / len(train_loader)}')\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in validation_loader:\n",
    "                x, y = x.to(CONFIG.device), y.to(CONFIG.device)\n",
    "                loss = model.loss(x, y)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "        validation_loss.append(running_loss / len(validation_loader))\n",
    "        print(f'Validation Loss: {running_loss / len(validation_loader)}')\n",
    "        \n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "1b495cc61a877c49",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:53:17.195117Z",
     "start_time": "2024-10-10T05:52:59.833789Z"
    }
   },
   "cell_type": "code",
   "source": "train(megabyte)",
   "id": "3d97d7626c626591",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 60/55706 [00:16<4:17:55,  3.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train(megabyte)\n",
      "Cell \u001B[0;32mIn[27], line 16\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model)\u001B[0m\n\u001B[1;32m     14\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mloss(x, y)\n\u001B[0;32m---> 16\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     17\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     18\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    520\u001B[0m     )\n\u001B[0;32m--> 521\u001B[0m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mbackward(\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs\u001B[38;5;241m=\u001B[39minputs\n\u001B[1;32m    523\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 289\u001B[0m _engine_run_backward(\n\u001B[1;32m    290\u001B[0m     tensors,\n\u001B[1;32m    291\u001B[0m     grad_tensors_,\n\u001B[1;32m    292\u001B[0m     retain_graph,\n\u001B[1;32m    293\u001B[0m     create_graph,\n\u001B[1;32m    294\u001B[0m     inputs,\n\u001B[1;32m    295\u001B[0m     allow_unreachable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    296\u001B[0m     accumulate_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    297\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    770\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    771\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "e70edc36e643819e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-10T05:53:31.932685Z",
     "start_time": "2024-10-10T05:53:18.808348Z"
    }
   },
   "cell_type": "code",
   "source": "megabyte.generate(sample_x, decode_fn=decode)",
   "id": "c5067e98aebbc674",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ǩh\u0004 ie  h  hh  h  hh  h  hh  h  hh  h  hh  h  hh  h  hh  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m megabyte\u001B[38;5;241m.\u001B[39mgenerate(sample_x, decode_fn\u001B[38;5;241m=\u001B[39mdecode)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/torch-env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "Cell \u001B[0;32mIn[24], line 45\u001B[0m, in \u001B[0;36mMEGABYTE.generate\u001B[0;34m(self, x, max_len, decode_fn)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# Decode token\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decode_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 45\u001B[0m     decoded_token \u001B[38;5;241m=\u001B[39m decode_fn([next_token[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()])\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28mprint\u001B[39m(decoded_token, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, flush\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluation",
   "id": "495822436e557079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "acd82062c9902c0f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
